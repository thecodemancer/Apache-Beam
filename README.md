# Apache-Beam

<img src="https://res.cloudinary.com/dxnufruex/image/upload/v1669761065/macrometa-web/images/6172fb248a6212d910d87b73_uLzn5MjM55jF0rj3YNgfPwakSo6-Vbng98ywy7mykWutqOhXP20PJsRfzFJlVg986fFWAjyzTErvoY5g32Vu60ui7Qgea2Qe1ReS3nlZt7czefTQ4QWLnpX1wDqYQznrffSW__08_s1600.png" />

¬°Bienvenidos, data practitioners! Durante los pr√≥ximos meses, nos embarcaremos en un emocionante viaje al mundo de Apache Beam, el framework open source l√≠der para construir e implementar data pipelines tanto en modo batch como en modo streaming utilizando una sola API y ejecut√°ndolo en cualquiera de los runners que dan soporte (Apache Spark, Apache Flink, Google Dataflow, Apache Apex, etc). Ya seas un principiante absoluto o tengas algo de experiencia en programaci√≥n, encontrar√°s este contenido fascinante y pr√°ctico.

## Lo que aprender√°s

- Aprender Apache Beam con su implementaci√≥n en tiempo real.
- Crear canales de procesamiento de datos empresariales en tiempo real utilizando Apache Beam.
- Aprender un modelo de programaci√≥n port√°til que puedes ejecutar en Spark, Flink, GCP (Google Cloud Dataflow), etc.
- Comprende el funcionamiento de todos y cada uno de los componentes de Apache Beam con ejercicios pr√°cticos.
- Desarrollar pipelines para Big Data del mundo real en diversos dominios comerciales.
- Carga de datos a tablas de Google BigQuery desde data pipelines hechos con Apache Beam.

## Estructura:

Se contar√° con sesiones semanales, cada una de una hora de duraci√≥n. Nos centraremos en:

- Caracter√≠sticas y M√©todos: Exploraremos las caracter√≠sticas y m√©todos principales del Framework, incluyendo ParDo, DoFn, etc.
- Conceptos: Cubriremos los conceptos clave en Apache Beam, como PCollection, PTransform.
- Aplicaciones: Nos adentraremos en las aplicaciones del mundo real de Apache Beam, con ejemplos para tiendas E-Commerce, banca, telecomunicaciones.

## Prerrequisitos

- [Python-StudyClub Data Engineering Latam üêç](https://github.com/DataEngineering-LATAM/Python-StudyClub)

## Contenido


### Beginners

#### Quickstarts

- [Java Quickstart](https://beam.apache.org/get-started/quickstart-java/) - How to set up and run a WordCount pipeline on the Java SDK.
- [Python Quickstart](https://beam.apache.org/get-started/quickstart-py/) - How to set up and run a WordCount pipeline on the Python SDK.
- [Go Quickstart](https://beam.apache.org/get-started/quickstart-go/) - How to set up and run a WordCount pipeline on the Go SDK.
- [Java Development Environment](https://medium.com/google-cloud/setting-up-a-java-development-environment-for-apache-beam-on-google-cloud-platform-ec0c6c9fbb39) - Setting up a Java development environment for Apache Beam using IntelliJ and Maven.
- [Python Development Environment](https://medium.com/google-cloud/python-development-environments-for-apache-beam-on-google-cloud-platform-b6f276b344df) - Setting up a Python development environment for Apache Beam using PyCharm.

#### Introduction

#### Conceptos

##### Runners

###### Overview

Apache Beam proporciona una capa API port√°til para crear sofisticados data pipelines de datos en paralelo que pueden ejecutarse en una diversidad de motores de ejecuci√≥n o ejecutores. Los conceptos centrales de esta capa se basan en el modelo Beam (anteriormente denominado modelo Dataflow) y se implementan en distintos grados en cada runner de Beam.

###### Direct runner

Direct Runner ejecuta data pipelines en tu m√°quina y est√° dise√±ado para validar que los data pipelines se adhieran al modelo Apache Beam lo m√°s fielmente posible. En lugar de centrarse en la ejecuci√≥n eficiente del data pipeline, Direct Runner realiza comprobaciones adicionales para garantizar que los usuarios no dependan de una sem√°ntica que no est√© garantizada por el modelo. Algunas de estas comprobaciones incluyen: 

- hacer cumplir la inmutabilidad de los elementos 
- hacer cumplir la codificabilidad de los elementos 
- los elementos se procesan en un orden arbitrario en todos los puntos 
- serializaci√≥n de las funciones del usuario (DoFn, CombineFn, etc.) 

El uso de Direct Runner para pruebas y desarrollo ayuda a garantizar que los data pipelines sean robustos en diferentes runners de Beam. Adem√°s, la depuraci√≥n de ejecuciones fallidas puede ser una tarea no trivial cuando un data pipeline se ejecuta en un cl√∫ster remoto. En cambio, suele ser m√°s r√°pido y sencillo realizar pruebas unitarias locales en el c√≥digo de tu data pipeline. La prueba unitaria local de tu data pipeline tambi√©n permite usar tus herramientas de depuraci√≥n locales preferidas. En el SDK de Python, el valor predeterminado para un runner es DirectRunner.

**Ejemplo**

```
python -m apache_beam.examples.wordcount --input YOUR_INPUT_FILE --output counts
```

###### Google Cloud Dataflow runner

Este runner utiliza los servicios administrados de Cloud Dataflow. cuando corres tu data pipeline con el servicio de Cloud Dataflow, el runner sube tu c√≥digo ejecutable y las dependencias a un bucket de Google Cloud Storage  y crea un job de Cloud Dataflow, el cual ejecuta tu pipeline con recursos administrados en Google Cloud Platform. El runner y el servicio de Cloud Dataflow son adecuados para jobs continuos a gran escala y proveen:

- un servicio totalmente administrado
- autoescalado del n√∫mero de workers a trav√©s del tiempo de vida del job
- Rebalanceo din√°mico de carga

#### Common Transforms

#### Core Transforms


#### Windowing


#### Triggers

#### IO Connectors

#### Splittable doFn

#### Cross-Language Transforms

#### Ejemplos

### Ejercicios

## Glosario de T√©rminos


## Otros Notebooks


## Libros Recomendados


## Playlists Recomendadas


## Certificaci√≥n
