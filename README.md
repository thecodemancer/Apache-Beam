# Apache-Beam

<img src="https://res.cloudinary.com/dxnufruex/image/upload/v1669761065/macrometa-web/images/6172fb248a6212d910d87b73_uLzn5MjM55jF0rj3YNgfPwakSo6-Vbng98ywy7mykWutqOhXP20PJsRfzFJlVg986fFWAjyzTErvoY5g32Vu60ui7Qgea2Qe1ReS3nlZt7czefTQ4QWLnpX1wDqYQznrffSW__08_s1600.png" />

춰Bienvenidos, data practitioners! Durante los pr칩ximos meses, nos embarcaremos en un emocionante viaje al mundo de Apache Beam, el framework open source l칤der para construir e implementar data pipelines tanto en modo batch como en modo streaming utilizando una sola API y ejecut치ndolo en cualquiera de los runners que dan soporte (Apache Spark, Apache Flink, Google Dataflow, Apache Apex, etc). Ya seas un principiante absoluto o tengas algo de experiencia en programaci칩n, encontrar치s este contenido fascinante y pr치ctico.

## Lo que aprender치s

- Aprender Apache Beam con su implementaci칩n en tiempo real.
- Crear canales de procesamiento de datos empresariales en tiempo real utilizando Apache Beam.
- Aprender un modelo de programaci칩n port치til que puedes ejecutar en Spark, Flink, GCP (Google Cloud Dataflow), etc.
- Comprende el funcionamiento de todos y cada uno de los componentes de Apache Beam con ejercicios pr치cticos.
- Desarrollar pipelines para Big Data del mundo real en diversos dominios comerciales.
- Carga de datos a tablas de Google BigQuery desde data pipelines hechos con Apache Beam.

## Estructura:

Se contar치 con sesiones semanales, cada una de una hora de duraci칩n. Nos centraremos en:

- Caracter칤sticas y M칠todos: Exploraremos las caracter칤sticas y m칠todos principales del Framework, incluyendo ParDo, DoFn, etc.
- Conceptos: Cubriremos los conceptos clave en Apache Beam, como PCollection, PTransform.
- Aplicaciones: Nos adentraremos en las aplicaciones del mundo real de Apache Beam, con ejemplos para tiendas E-Commerce, banca, telecomunicaciones.

## Prerrequisitos

- [Python-StudyClub Data Engineering Latam 游냀](https://github.com/DataEngineering-LATAM/Python-StudyClub)

## Contenido

### Quickstarts

- [Java Quickstart](https://beam.apache.org/get-started/quickstart-java/) - How to set up and run a WordCount pipeline on the Java SDK.
- [Python Quickstart](https://beam.apache.org/get-started/quickstart-py/) - How to set up and run a WordCount pipeline on the Python SDK.
- [Go Quickstart](https://beam.apache.org/get-started/quickstart-go/) - How to set up and run a WordCount pipeline on the Go SDK.
- [Java Development Environment](https://medium.com/google-cloud/setting-up-a-java-development-environment-for-apache-beam-on-google-cloud-platform-ec0c6c9fbb39) - Setting up a Java development environment for Apache Beam using IntelliJ and Maven.
- [Python Development Environment](https://medium.com/google-cloud/python-development-environments-for-apache-beam-on-google-cloud-platform-b6f276b344df) - Setting up a Python development environment for Apache Beam using PyCharm.

### Introduction

#### Conceptos

##### Runners

###### Overview

Apache Beam proporciona una capa API port치til para crear sofisticados data pipelines de datos en paralelo que pueden ejecutarse en una diversidad de motores de ejecuci칩n o ejecutores. Los conceptos centrales de esta capa se basan en el modelo Beam (anteriormente denominado modelo Dataflow) y se implementan en distintos grados en cada runner de Beam.

###### Direct runner

Direct Runner ejecuta data pipelines en tu m치quina y est치 dise침ado para validar que los data pipelines se adhieran al modelo Apache Beam lo m치s fielmente posible. En lugar de centrarse en la ejecuci칩n eficiente del data pipeline, Direct Runner realiza comprobaciones adicionales para garantizar que los usuarios no dependan de una sem치ntica que no est칠 garantizada por el modelo. Algunas de estas comprobaciones incluyen: 

- hacer cumplir la inmutabilidad de los elementos 
- hacer cumplir la codificabilidad de los elementos 
- los elementos se procesan en un orden arbitrario en todos los puntos 
- serializaci칩n de las funciones del usuario (DoFn, CombineFn, etc.) 

El uso de Direct Runner para pruebas y desarrollo ayuda a garantizar que los data pipelines sean robustos en diferentes runners de Beam. Adem치s, la depuraci칩n de ejecuciones fallidas puede ser una tarea no trivial cuando un data pipeline se ejecuta en un cl칰ster remoto. En cambio, suele ser m치s r치pido y sencillo realizar pruebas unitarias locales en el c칩digo de tu data pipeline. La prueba unitaria local de tu data pipeline tambi칠n permite usar tus herramientas de depuraci칩n locales preferidas. En el SDK de Python, el valor predeterminado para un runner es DirectRunner.

**Ejemplo**

```
python -m apache_beam.examples.wordcount --input YOUR_INPUT_FILE --output counts
```

###### Google Cloud Dataflow runner

Este runner utiliza los servicios administrados de Cloud Dataflow. cuando corres tu data pipeline con el servicio de Cloud Dataflow, el runner sube tu c칩digo ejecutable y las dependencias a un bucket de Google Cloud Storage  y crea un job de Cloud Dataflow, el cual ejecuta tu pipeline con recursos administrados en Google Cloud Platform. El runner y el servicio de Cloud Dataflow son adecuados para jobs continuos a gran escala y proporcionan:

- un servicio totalmente administrado
- autoescalado del n칰mero de workers a trav칠s del tiempo de vida del job
- Rebalanceo din치mico de carga

**Ejemplo**

```
# Como parte de la configuraci칩n inicial, se instalan los componentes espec칤ficos extra de Google Cloud Platform.

pip install apache-beam[gcp]
python -m apache_beam.examples.wordcount --input gs://dataflow-samples/shakespeare/kinglear.txt \
                                         --output gs://YOUR_GCS_BUCKET/counts \
                                         --runner DataflowRunner \
                                         --project YOUR_GCP_PROJECT \
                                         --region YOUR_GCP_REGION \
                                         --temp_location gs://YOUR_GCS_BUCKET/tmp/
```

###### Apache Flink runner

El runner de Apache Flink se puede utilizar para ejecutar data pipelines de Beam utilizando Apache Flink. Para la ejecuci칩n, puedes elegir entre un modo de ejecuci칩n en cl칰ster (por ejemplo, Yarn/Kubernetes/Mesos) o un modo de ejecuci칩n integrado local que es 칰til para hacer pruebas. El runner de Flink y Apache Flink son adecuados para job continuos a gran escala y proporcionan:

- Un runtime centrado en streaming que admite programas de procesamiento por lotes y en streaming de datos
- Un runtime que admite un rendimiento muy alto y una baja latencia de eventos al mismo tiempo
- Tolerancia a fallos con garant칤as de procesamiento exactamente una vez
- Contrapresi칩n natural en programas de streaming.
- Gesti칩n de memoria personalizada para una conmutaci칩n eficiente y s칩lida entre algoritmos de procesamiento de datos en memoria y fuera del n칰cleo
- Integraci칩n con YARN y otros componentes del ecosistema Apache Hadoop

**Ejemplo**

1. A partir de Beam 2.18.0, im치genes de Docker preconstruidas del servicio Flink est치n disponibles en Docker Hub: `Flink 1.10, Flink 1.11, Flink 1.12, Flink 1.13, Flink 1.14.2.`
2. Inicializa el endpoint JobService: `docker run --net=host apache/beam_flink1.10_job_server:latest3`
3. Env칤a el pipeline al endpoint de arriba utilizando el PortableRunner, job_endpoint configurado para localhost:8099 (esta es la direcci칩n por defecto del JobService). Opcionalmente setea el environment_type a LOOPBACK. Por ejemplo:
   
```
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

options = PipelineOptions([
    "--runner=PortableRunner",
    "--job_endpoint=localhost:8099",
    "--environment_type=LOOPBACK"
])
with beam.Pipeline(options) as p:
    ...
```
    
#### Common Transforms

#### Core Transforms


#### Windowing


#### Triggers

#### IO Connectors

#### Splittable doFn

#### Cross-Language Transforms

#### Ejemplos

### Ejercicios

## Glosario de T칠rminos


## Otros Notebooks


## Libros Recomendados


## Playlists Recomendadas


## Certificaci칩n
